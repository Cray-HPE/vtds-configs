#
# MIT License
#
# (C) Copyright 2025 Hewlett Packard Enterprise Development LP
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
#
# Cluster layer configuration for deploying OpenCHAMI on vTDS
cluster:
  networks:
    management_network:
      network_name: mgmtnet
      tunnel_id: 10
      devices:
        bridge: br-mgmtnet
        tunnel: mgmtnet
        local:
          peer: bl-mgmtnet-p
          interface: bl-mgmtnet
      blade_interconnect: blade-interconnect
      address_families:
        ipv4:
          family: AF_INET
          cidr: 10.1.1.0/24
          gateway: 10.1.1.1
          name_servers:
            - 169.254.169.254
          connected_blades:
            - blade_class: host-blade
              addresses:
                - 10.1.1.1
              dhcp_server_instance: 0
          dhcp:
            enabled: true
            pools:
              - start: 10.1.1.10
                end: 10.1.1.254
    compute_network:
      network_name: compnet
      tunnel_id: 20
      devices:
        bridge: br-compnet
        tunnel: compnet
        local:
          peer: bl-compnet-p
          interface: bl-compnet
      blade_interconnect: blade-interconnect
      address_families:
        ipv4:
          family: AF_INET
          cidr: 10.2.1.0/24
          gateway: 10.2.1.1
          name_servers:
            - 169.254.169.254
          connected_blades:
            - blade_class: host-blade
              addresses:
                - 10.2.1.1
              dhcp_server_instance: 0
          dhcp:
            enabled: true
            pools:
              - start: 10.2.1.10
                end: 10.2.1.254
    hardware_network:
      network_name: hwmgtnet
      tunnel_id: 30
      devices:
        bridge: br-hwmgtnet
        tunnel: hwmgtnet
        local:
          peer: bl-hwmgtnet-p
          interface: bl-hwmgtnet
      blade_interconnect: blade-interconnect
      address_families:
        ipv4:
          family: AF_INET
          cidr: 10.3.1.0/24
          gateway: null
          name_servers:
            - 169.254.169.254
          connected_blades:
            - blade_class: host-blade
              addresses:
                - 10.3.1.1
                - 10.3.1.2
                - 10.3.1.3
                - 10.3.1.4
                - 10.3.1.5
                - 10.3.1.6
                - 10.3.1.7
                - 10.3.1.8
                - 10.3.1.9
                - 10.3.1.10
                - 10.3.1.11
                - 10.3.1.12
                - 10.3.1.13
                - 10.3.1.14
                - 10.3.1.15
                - 10.3.1.16
              dhcp_server_instance: 0
          dhcp:
            enabled: true
            pools:
              - start: 10.3.1.16
                end: 10.3.1.254
  node_classes:
    management_node:
      pure_base_class: false
      parent_class: rocky_linux_node
      node_count: 1
      virtual_machine:
        cpu_count: 2  # Want a little more CPU than the minimum
      host_blade:
        blade_class: host-blade
        instance_capacity: 1
      node_naming:
        base_name: management
        # In this configuration, there is one management node and it
        # lives on the same node as the first 4 compute nodes. Because
        # of this, the cabinet (x), chassis (c), shelf (s), and
        # blade (b) numbers need to match those of the compute nodes
        # and it will show up as the first node on the blade (n0)
        node_names:
          - x1c0s0b0n0
      host_naming:
        base_name: management
      network_interfaces:
        management_network:
          cluster_network: mgmtnet
          addr_info:
            ipv4:
              family: AF_INET
              mode: dynamic
        hardware_network:
          cluster_network: hwmgtnet
          addr_info:
            ipv4:
              family: AF_INET
              mode: dynamic
    compute_node:
      pure_base_class: false
      parent_class: rocky_linux_node
      # No compute nodes by default. Increase this number to have
      # compute nodes in the system. Since the host blade capacity is 4
      # compute nodes, if you add more than 4 compute nodes, you will
      # need to increase the number of host blades accordingly in the
      # provider configuration.
      node_count: 0
      virtual_machine:
        cpu_count: 1
      host_blade:
        blade_class: host-blade
        instance_capacity: 4
      node_naming:
        base_name: compute
        # On OpenCHAMI, node names are xnames because that is
        # convenient.
        #
        # The xnames for the first 4 compute nodes are offset by one
        # from the normal pattern to acommodate the management node that
        # is co-resident on the same blade with those compute
        # nodes. After that, each group of 4 compute nodes (n0 through
        # n3) lives on a blade (b0) in its own shelf (s1 through
        # s15). Currently only the first 16 compute nodes are reflected
        # here. Follow the pattern to extened this up to 64 nodes if
        # desired.
        #
        # This needs to be coordinated with the application metadata in
        # the provider layer config to make sure Blade xnames match up
        # with node xnames. Blade xnames are currently defined there up
        # to 16 blades (support for 64 nodes)
        node_names:
          - x1c0s0b0n1
          - x1c0s0b0n2
          - x1c0s0b0n3
          - x1c0s0b0n4
          - x1c0s1b0n0
          - x1c0s1b0n1
          - x1c0s1b0n2
          - x1c0s1b0n3
          - x1c0s2b0n0
          - x1c0s2b0n1
          - x1c0s2b0n2
          - x1c0s2b0n3
          - x1c0s3b0n0
          - x1c0s3b0n1
          - x1c0s3b0n2
          - x1c0s3b0n3
      host_naming:
        base_name: compute
      network_interfaces:
        compute_network:
          cluster_network: compnet
          addr_info:
            ipv4:
              family: AF_INET
              mode: dynamic
